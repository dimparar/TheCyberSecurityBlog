<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Docs on The Cyber Security Blog</title>
    <link>https://dimparar.github.io/thecybersecurityblog/docs/</link>
    <description>Recent content in Docs on The Cyber Security Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 08 Nov 2024 13:43:52 +0200</lastBuildDate>
    <atom:link href="https://dimparar.github.io/thecybersecurityblog/docs/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Hack The Box: PermX</title>
      <link>https://dimparar.github.io/thecybersecurityblog/docs/htb_permx/</link>
      <pubDate>Fri, 08 Nov 2024 13:43:52 +0200</pubDate>
      <guid>https://dimparar.github.io/thecybersecurityblog/docs/htb_permx/</guid>
      <description>Let&amp;rsquo;s scan the machine with nmap.
# Command sudo nmap -sS -sV -sC -v -T4 10.10.11.23 # Output PORT STATE SERVICE VERSION 22/tcp open ssh OpenSSH 8.9p1 Ubuntu 3ubuntu0.10 (Ubuntu Linux; protocol 2.0) | ssh-hostkey: | 256 e2:5c:5d:8c:47:3e:d8:72:f7:b4:80:03:49:86:6d:ef (ECDSA) |_ 256 1f:41:02:8e:6b:17:18:9c:a0:ac:54:23:e9:71:30:17 (ED25519) 80/tcp open http Apache httpd 2.4.52 Let&amp;rsquo;s check the Http Server in browser. It seems then we try to access http://10.10.11.23:80/ we get redirected to http://permx.htb/, but the page doesn&amp;rsquo;t load.</description>
    </item>
    <item>
      <title>Hack The Box: Cap</title>
      <link>https://dimparar.github.io/thecybersecurityblog/docs/htb_cap/</link>
      <pubDate>Fri, 01 Nov 2024 22:59:51 +0200</pubDate>
      <guid>https://dimparar.github.io/thecybersecurityblog/docs/htb_cap/</guid>
      <description>Let ip_address=10.10.10.10.
Scan the machine with nmap.
sudo nmap -sS -sV -sC -T4 -v -Pn 10.10.10.10 Host discovery disabled (-Pn). All addresses will be marked &amp;#39;up&amp;#39; and scan times may be slower. Starting Nmap 7.94SVN ( https://nmap.org ) at 2024-11-01 14:55 EDT NSE: Loaded 156 scripts for scanning. NSE: Script Pre-scanning. Initiating NSE at 14:55 Completed NSE at 14:55, 0.00s elapsed Initiating NSE at 14:55 Completed NSE at 14:55, 0.00s elapsed Initiating NSE at 14:55 Completed NSE at 14:55, 0.</description>
    </item>
    <item>
      <title>PortSwigger Labs</title>
      <link>https://dimparar.github.io/thecybersecurityblog/docs/portswigger_labs/</link>
      <pubDate>Tue, 29 Oct 2024 22:01:23 +0200</pubDate>
      <guid>https://dimparar.github.io/thecybersecurityblog/docs/portswigger_labs/</guid>
      <description>Introduction Lab: Web shell upload via Content-Type restriction bypass In this lab we have to exploit an upload vulnerability.
Go to &amp;ldquo;My account&amp;rdquo; and log in with the given credentials Username: wiener
Password: peter
After logging in, we get redirected to &amp;ldquo;My account&amp;rdquo; page where there is an input box to upload our Avatar photo. We want to check if there is any upload vulnerability we can exploit to get access to the server.</description>
    </item>
    <item>
      <title>Crackmes.one Challenges</title>
      <link>https://dimparar.github.io/thecybersecurityblog/docs/crackmes-challenges/</link>
      <pubDate>Wed, 13 Mar 2024 22:10:50 +0200</pubDate>
      <guid>https://dimparar.github.io/thecybersecurityblog/docs/crackmes-challenges/</guid>
      <description> Challenge Difficulty Quality Link sporta778&#39;s Bobby 2.0 5.0 here </description>
    </item>
    <item>
      <title>Bank CTF</title>
      <link>https://dimparar.github.io/thecybersecurityblog/docs/bank_ctf/</link>
      <pubDate>Wed, 13 Mar 2024 01:23:32 +0200</pubDate>
      <guid>https://dimparar.github.io/thecybersecurityblog/docs/bank_ctf/</guid>
      <description>Link:TryHackMe | Bank CTF
service apache2 start -&amp;gt; start apache2 server
service apache2 stop -&amp;gt; stop apache2 server
wget -&amp;gt; download files from internet
Web Crawler A web crawler, or spider, isÂ a type of bot that is typically operated by search engines like Google and Bing. Their purpose is to index the content of websites all across the Internet so that those websites can appear in search engine results</description>
    </item>
  </channel>
</rss>
